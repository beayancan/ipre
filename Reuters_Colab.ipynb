{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FOnO1SD-pzs"
   },
   "source": [
    "# Ejemplo NMF Rank-2 con dataset Reuters\n",
    "\n",
    "Autor: Benjamin Ayancán, PUC Chile\n",
    "Tutor: Denis Parra, PUC Chile\n",
    "\n",
    "En el siguiente notebook desarrollaremos un ejemplo de cómo poder generar una descomposición jerarquica a través del algoritmo de rango bajo NMF rank-2.\n",
    "\n",
    "* Para este tutorial usaremos el dataset de noticias Reuters que nos entrega la libería `keras`\n",
    "\n",
    "* Los aspectos teoricos y detalles del algoritmo son abordados en el  siguiente [notebook de Observablehq](https://observablehq.com/@beayancan/descomposicion-jerarquica)\n",
    "\n",
    "* Para hacer más pedagógico el ejemplo se va a reducir tanto la cantidad de datos (filas) como la cantidad de features (columnas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLET-ev4s9Rp"
   },
   "source": [
    "---\n",
    "\n",
    "## Configuración\n",
    "\n",
    "### Notebooks\n",
    "\n",
    "--> Si lo estás ejecutando en un `jupyter notebook` debes instalar las siguientes librerías\n",
    "\n",
    "```py\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "```\n",
    "\n",
    "--> Si lo estás ejecutando en `colab`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEb5CzWsY-xO"
   },
   "outputs": [],
   "source": [
    "# Chequeamos que nuestro soporte no tenga problemas\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hb3X252TY0H2",
    "outputId": "1bb6d096-4898-4c6d-995a-462ac45e6fc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importamos las liberías que vamos a utilizar\n",
    "# herramientas y el dataset\n",
    "\n",
    "import os, sys\n",
    "import keras\n",
    "import statistics\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPD06Xvdtbp7"
   },
   "source": [
    "---\n",
    "\n",
    "## Reuters\n",
    "\n",
    "#### ¿Por qué usar esta librería?\n",
    "\n",
    "Reuters es un repositorio de documentos que podemos utilizar a través de `keras` o bien descargarlo en su [página web](http://konect.cc/networks/gottron-reuters/). Esta posee las siguientes características\n",
    "\n",
    "* Son un conjunto de 11.228 noticias, etiquetados en 46 tópicos.\n",
    "\n",
    "\n",
    "* Está preprocesado según un ranking de palabras de un volabulario, es decir su representación es a través de las posiciones en el ranking de las palabras de un documento\n",
    "* Posee funciones sencillas que nos ayudan a manejar cómo representar los documentos sin gastar mucho tiempo en su preprocesamiento\n",
    "* Se enfoca en el contenido de un documento, las palabras que lo representan, en vez del mensaje que este entregue\n",
    "\n",
    "---\n",
    "\n",
    "#### Cargar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBBfF4DIuHC-"
   },
   "outputs": [],
   "source": [
    "# PARAMETROS\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters/load_data\n",
    "\n",
    "# Cantidad de palabras significativas del vocabulario a usar\n",
    "\n",
    "# num_words = None # incluirlas todas\n",
    "num_words = 100 # solo tomaremos las 100 primeras palabras\n",
    "skip_top = 0 # palabras que no consideraremos\n",
    "\n",
    "# máximo N° de palabras para representar a un documento\n",
    "max_len = None # todas\n",
    "# max_len = 50\n",
    "\n",
    "# Porcentaje de palabras para usar en el test set\n",
    "test_split = 0.025 # train set: 97.5%, test set: 2.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "MPNg4lWueWs9",
    "outputId": "cb6c810e-43e2-49e0-9614-3cee8e1a852c"
   },
   "outputs": [],
   "source": [
    "# Cargamos los de datos de clasificación de noticias de Reuters\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words,\n",
    "                                                         maxlen=max_len,\n",
    "                                                         test_split=test_split,\n",
    "                                                         skip_top=skip_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "xrzJdAjwWnvA",
    "outputId": "cf86be6d-256b-4a1c-95f6-8143ea984288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño train set: (10947,)\n",
      "Tamaño test set: (281,)\n",
      "Cantidad clases: 46\n",
      "Palabra mayor 2\n",
      "Oalabra menor 2376\n"
     ]
    }
   ],
   "source": [
    "# Revisamos cómo fueron cargados los datos\n",
    "\n",
    "# y_train, y_test contendrán los labels de las clases\n",
    "\n",
    "num_clases = max(y_train) + 1\n",
    "\n",
    "print(f'Tamaño train set: {x_train.shape}')\n",
    "print(f'Tamaño test set: {x_test.shape}')\n",
    "print(f'Cantidad clases: {num_clases}')\n",
    "print('Palabra mayor', min(len(e) for e in x_train))\n",
    "print('Oalabra menor', max(len(e) for e in x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tW0E5KIwiCi"
   },
   "source": [
    "### Qué es lo que contienen\n",
    "* Cada elemento del set contiene los índices de las palabras más utilizadas por un documento\n",
    "\n",
    "* Las palabras están ordenadas según su frecuencia de aparición\n",
    "* Además cada documento pertenece a un tópico en especifico\n",
    "* Notar por el `x_train.shape` que las representaciones no tienen un largo especifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "2SqXKHd1gxs8",
    "outputId": "c23a4019-c6e0-43ef-e894-2c6e51c8eaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 1,  largo: 88, clase 20\n",
      "   contenido: [1, 4, 2, 2, 2, 5, 2, 28, 4, 2, 37, 2, 2, 2, 2, 2, 6, 72, 2, 20, 22, 72, 2, 20, 54, 2, 2, 28, 4, 2, 2, 96, 5, 2, 2, 2, 62, 2, 2, 2, 20, 5, 4, 2, 2, 13, 9, 2, 2, 24, 96, 62, 2, 7, 2, 4, 60, 5, 2, 8, 10, 2, 5, 2, 11, 2, 5, 2, 2, 34, 2, 13, 2, 52, 2, 54, 2, 4, 2, 2, 5, 2, 34, 72, 2, 20, 17, 12] \n",
      "\n",
      "Doc: 4,  largo: 85, clase 4\n",
      "   contenido: [1, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 8, 36, 23, 2, 6, 2, 2, 18, 2, 79, 5, 2, 2, 71, 28, 2, 15, 2, 36, 8, 4, 2, 2, 5, 4, 2, 2, 79, 36, 2, 2, 41, 2, 2, 2, 44, 20, 5, 2, 2, 2, 2, 79, 2, 2, 8, 36, 91, 2, 95, 9, 2, 2, 2, 79, 36, 8, 4, 2, 9, 2, 2, 33, 2, 28, 2, 2, 2, 2, 94, 2, 89, 17, 12] \n",
      "\n",
      "Doc: 7,  largo: 49, clase 3\n",
      "   contenido: [1, 53, 2, 26, 14, 2, 26, 39, 32, 2, 18, 14, 12, 2, 18, 88, 2, 19, 11, 14, 2, 72, 11, 2, 2, 53, 2, 26, 14, 2, 26, 39, 44, 2, 18, 14, 44, 2, 18, 88, 2, 59, 11, 14, 2, 63, 11, 17, 12] \n",
      "\n",
      "Doc: 10,  largo: 286, clase 16\n",
      "   contenido: [1, 4, 2, 2, 2, 2, 2, 2, 28, 2, 7, 2, 40, 85, 2, 2, 2, 4, 2, 9, 2, 24, 2, 2, 4, 2, 28, 4, 2, 10, 2, 2, 8, 36, 8, 4, 2, 5, 4, 2, 2, 2, 2, 2, 4, 2, 18, 2, 2, 2, 34, 2, 4, 2, 2, 34, 2, 2, 2, 51, 10, 2, 2, 6, 2, 4, 2, 2, 33, 30, 2, 7, 4, 2, 2, 5, 2, 36, 8, 51, 2, 34, 2, 2, 6, 4, 2, 2, 5, 4, 2, 18, 2, 2, 50, 2, 2, 65, 2, 6, 2, 9, 16, 33, 30, 2, 5, 2, 13, 2, 2, 4, 2, 8, 2, 33, 45, 2, 2, 2, 43, 10, 2, 5, 4, 2, 43, 16, 33, 30, 2, 6, 2, 7, 2, 22, 2, 2, 2, 40, 10, 2, 18, 2, 2, 7, 2, 10, 2, 2, 2, 2, 28, 2, 2, 2, 10, 2, 2, 2, 2, 28, 2, 7, 2, 9, 10, 2, 2, 2, 36, 8, 4, 2, 9, 2, 7, 50, 68, 2, 34, 2, 2, 2, 4, 2, 34, 45, 7, 2, 4, 2, 43, 10, 2, 64, 85, 2, 28, 69, 2, 5, 25, 2, 2, 4, 2, 5, 2, 2, 42, 2, 2, 34, 2, 2, 21, 4, 2, 2, 5, 4, 2, 2, 4, 2, 23, 2, 36, 8, 52, 2, 34, 2, 6, 30, 2, 27, 2, 93, 51, 52, 23, 2, 2, 2, 2, 4, 2, 2, 78, 2, 62, 2, 7, 42, 2, 9, 2, 7, 10, 2, 2, 28, 4, 2, 2, 21, 94, 2, 51, 4, 2, 8, 52, 2, 34, 2, 2, 2, 22, 4, 2, 2, 17, 12] \n",
      "\n",
      "Doc: 13,  largo: 38, clase 3\n",
      "   contenido: [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 2, 2, 2, 2, 49, 2, 25, 99, 2, 2, 5, 2, 44, 26, 2, 93, 2, 6, 2, 5, 2, 93, 19, 17, 12] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostremos algunos ejemplos\n",
    "for i in range(1,15, 3):\n",
    "  print(f'Doc: {i},  largo: {len(x_test[i])}, clase {y_test[i]}')\n",
    "  print(f'   contenido: {x_test[i]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 866
    },
    "colab_type": "code",
    "id": "TmYUZB3gtZBd",
    "outputId": "f60e3dcb-7302-4b79-fc36-f6a169c9616c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Las clases y sus estadísticas\n",
      "Idx  Clase                train  test  Palabras\n",
      "    0 cocoa                   64     3   236.69\n",
      "    1 grain                  526    11   187.91\n",
      "    2 veg-oil                 91     3   180.86\n",
      "    3 earn                  3867   105    87.05\n",
      "    4 acq                   2373    50   136.51\n",
      "    5 wheat                   22     0   188.41\n",
      "    6 copper                  61     1   154.18\n",
      "    7 housing                 19     0   166.26\n",
      "    8 money-supply           170     7   192.82\n",
      "    9 coffee                 125     1   215.36\n",
      "   10 sugar                  149     5   186.44\n",
      "   11 trade                  461    12   256.77\n",
      "   12 reserves                61     1   184.51\n",
      "   13 ship                   202     7   166.66\n",
      "   14 cotton                  28     0   143.11\n",
      "   15 carcass                 28     1   193.11\n",
      "   16 crude                  530    13   223.52\n",
      "   17 nat-gas                 49     2   161.08\n",
      "   18 cpi                     80     6   164.34\n",
      "   19 money-fx               669    13   196.57\n",
      "   20 interest               325    14   205.60\n",
      "   21 gnp                    124     3   274.73\n",
      "   22 meal-feed               21     1   171.86\n",
      "   23 alum                    51     2   149.59\n",
      "   24 oilseed                 79     2   146.28\n",
      "   25 gold                   120     3   160.02\n",
      "   26 tin                     32     0   270.41\n",
      "   27 strategic-metal         19     0   144.53\n",
      "   28 livestock               58     0   171.93\n",
      "   29 retail                  23     0   239.43\n",
      "   30 ipi                     54     3   183.94\n",
      "   31 iron-steel              50     2   153.24\n",
      "   32 rubber                  41     1   221.10\n",
      "   33 heat                    16     0   102.31\n",
      "   34 jobs                    57     0   164.53\n",
      "   35 lei                     15     1   141.53\n",
      "   36 bop                     58     2   229.81\n",
      "   37 zinc                    21     0   162.24\n",
      "   38 orange                  22     0   124.68\n",
      "   39 pet-chem                29     0   159.90\n",
      "   40 dlr                     43     3   265.16\n",
      "   41 gas                     38     0   165.45\n",
      "   42 silver                  15     1   210.60\n",
      "   43 wpi                     26     1   163.88\n",
      "   44 hog                     17     0   110.76\n",
      "   45 lead                    18     1   159.89\n"
     ]
    }
   ],
   "source": [
    "mapping = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "           'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "           'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "           'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "           'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']\n",
    "\n",
    "train_count = collections.Counter(y_train)\n",
    "test_count = collections.Counter(y_test)\n",
    "total_words = [statistics.mean([len(e) for e in x_train[y_train.flatten() == i]]) for i in range(46)]\n",
    "\n",
    "print(\"         Las clases y sus estadísticas\")\n",
    "print(\"{:4s} {:20s} {:5s}  {:5s} {:7s}\".format(\"Idx\",\"Clase\", \"train\", \"test\", \"Palabras\"))\n",
    "for i in range(46):\n",
    "    print(\"{:5d} {:20s} {:5d} {:5d}   {:6.2f}\".format(i,mapping[i], train_count[i], test_count[i], total_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1QfRta5-2UO"
   },
   "source": [
    "### Qué es lo que representan\n",
    "* Solo tenemos los índices, pero ¿de qué?\n",
    "  * Son la posición rankeada de la palabra\n",
    "* Usamos la indexación del vocabulario del dataset para identificar la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Ses2uu-X6EiC",
    "outputId": "6579106d-530b-4ea8-ff0e-ef9581941f4f"
   },
   "outputs": [],
   "source": [
    "# Tenemos el vocabulario con las palabras y sus indices\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "# Generamos un diccionario con el contenido\n",
    "index_to_word = { value+3 : key for key, value in word_index.items() }\n",
    "\n",
    "# Indices reservados\n",
    "index_to_word[0] = '-PAD-'   # 0: carpeta\n",
    "index_to_word[1] = '-START-' # 1: inicio secuencia\n",
    "index_to_word[2] = '-UNK-'   # 2: elemento no encontrado\n",
    "\n",
    "len_index = len(index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtkSV7UNPdc9"
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "* Para poder realizar los calculos, necesitamos\n",
    "  * Una estructura regular de datos\n",
    "  * Que cada documento posea un mismo largo\n",
    "\n",
    "* Pasaremos primero los datos a una menor dimensión reduciendo las palabras y eliminando lo innecesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjKzIVgY2M2U"
   },
   "outputs": [],
   "source": [
    "# Primero eliminaremos las referencias a elementos que no se encuentran\n",
    "# dentro de las 1000 palabras más usadas en el vocabulario\n",
    "# ademas de las entradas reservadas\n",
    "\n",
    "  # Eliminamos\n",
    "  # 0: '-PAD-'\n",
    "  # 1: '-START-'\n",
    "  # 2: '-UNK-'\n",
    "  # 12: '3'\n",
    "  # 17: 'reuter'\n",
    "\n",
    "def filtrar_relevante(arreglo, por_eliminar=[0,1,2]):\n",
    "  \"\"\"\n",
    "  Borra las palabras que pertenecen a los indices del array por_eliminar\n",
    "  \"\"\"\n",
    "  return list(filter(lambda x: x not in por_eliminar, arreglo))\n",
    "\n",
    "def eliminar_reservadas(x_array):\n",
    "  \"\"\"\n",
    "  Eliminamos las entradas reservadas y entradas inutiles\n",
    "  retorna el contenido homogeneo del doc\n",
    "  \"\"\"\n",
    "  por_eliminar = [0,1,2,12,17]\n",
    "  largo_test, = x_array.shape\n",
    "  for i in range(largo_test):\n",
    "    x_array[i] = filtrar_relevante(x_array[i], por_eliminar)\n",
    "  return x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVifRE33sxqN"
   },
   "outputs": [],
   "source": [
    "# Para hacer un ejemplo más sencillo de entender\n",
    "# Selecionaremos solo las primeras 7 clases\n",
    "\n",
    "def reducir_labels(data_array, labels, k=7):\n",
    "  \"\"\"\n",
    "  Filtramos los documentos que pertenezcan a las primeras k clases\n",
    "  Retornamos el arreglo con los documentos y sus correspondientes labels\n",
    "  \"\"\"\n",
    "\n",
    "  retorno, retorno_labels = list(), list()\n",
    "  for i in range(len(data_array)):\n",
    "    if labels[i] < k:\n",
    "      retorno.append(data_array[i])\n",
    "      retorno_labels.append(labels[i])\n",
    "  return np.array(retorno), retorno_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5NPalNU7tAXU",
    "outputId": "68f3ec51-ac70-4414-d91a-ae128e5848f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173,)\n"
     ]
    }
   ],
   "source": [
    "x_test = eliminar_reservadas(x_test)\n",
    "x_data, y_data = reducir_labels(x_test, y_test, k=7)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "MbLfYLj3vI2m",
    "outputId": "f4b03fcf-6dea-4458-e898-871d68b0ea4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 1,  Largo: 55, Clase 3\n",
      " [71, 8, 25, 10, 5, 68, 13, 67, 5, 4, 80, 6, 5, 93, 16, 8, 4, 6, 68, 5, 10, 67, 5, 80, 5, 28, 42, 96, 5, 15, 8, 4, 91, 30, 92, 83, 4, 5, 58, 5, 10, 13, 28, 20, 5, 4, 80, 4, 49, 24, 16, 40, 6, 23, 10] \n",
      " inc said its a of one for share of the stock to of april it said the to one of a share of stock of at an price of dlrs said the may be after 10 the of or of a for at pct of the stock the company that it has to is a \n",
      "\n",
      "Doc: 4,  Largo: 71, Clase 3\n",
      " [53, 19, 15, 14, 26, 39, 59, 11, 14, 32, 11, 86, 19, 35, 14, 19, 32, 35, 53, 15, 14, 32, 15, 39, 11, 14, 70, 11, 86, 63, 35, 14, 44, 35, 89, 9, 68, 92, 5, 11, 15, 58, 26, 10, 67, 13, 7, 4, 98, 5, 42, 7, 48, 39, 27, 26, 10, 67, 13, 7, 21, 89, 34, 72, 35, 15, 50, 49, 8, 34, 10] \n",
      " shr 1 dlrs vs cts net 6 mln vs 2 mln revs 1 billion vs 1 2 billion shr dlrs vs 2 dlrs net mln vs 0 mln revs 8 billion vs 5 billion 1987 and one after of mln dlrs or cts a share for in the quarter of an in 1986 net by cts a share for in on 1987 was 9 billion dlrs which company said was a \n",
      "\n",
      "Doc: 7,  Largo: 50, Clase 3\n",
      " [53, 46, 68, 14, 74, 26, 39, 46, 70, 11, 14, 74, 18, 86, 59, 11, 14, 11, 29, 53, 46, 32, 15, 14, 46, 32, 15, 39, 46, 63, 47, 11, 14, 46, 61, 72, 11, 86, 11, 14, 32, 11, 48, 29, 68, 5, 19, 32, 11, 15] \n",
      " shr loss one vs profit cts net loss 0 mln vs profit 000 revs 6 mln vs mln year shr loss 2 dlrs vs loss 2 dlrs net loss 8 4 mln vs loss 7 9 mln revs mln vs 2 mln 1986 year one of 1 2 mln dlrs \n",
      "\n",
      "Doc: 10,  Largo: 12, Clase 3\n",
      " [10, 49, 25, 99, 5, 44, 26, 93, 6, 5, 93, 19] \n",
      " a company its first of 5 cts april to of april 1 \n",
      "\n",
      "Doc: 13,  Largo: 4, Clase 3\n",
      " [26, 14, 26, 94] \n",
      " cts vs cts march \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostremos lo que contiene\n",
    "for i in range(1,15, 3):\n",
    "  print(f'Doc: {i},  Largo: {len(x_data[i])}, Clase {y_data[i]}')\n",
    "  print(f' {x_data[i]}', '\\n', ' '.join([index_to_word[word] for word in x_data[i]]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VV9CKo3L8Get"
   },
   "source": [
    "### Pasar los datos a matriz doc-term\n",
    "\n",
    "* Pasamos a una matriz $A \\in \\mathbb{R}^{m \\times n}$ donde $m$ es la cantidad de documentos y $n$ es la cantidad de palabras del vocabulario\n",
    "* Pasaremos los datos a una representación de $\\{0, 1\\}$\n",
    "* Representando la entrada $A[i,j]$ la aparición en el $i$-ésimo documento la $j$-ésima palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ngXLKqz6etw"
   },
   "outputs": [],
   "source": [
    "# Importamos tokenizer para producirlo\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tkn = Tokenizer(num_words=num_words) # tamaño del vocabulario\n",
    "num_clases = max(y_data) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2pvAU85QIr6"
   },
   "outputs": [],
   "source": [
    "# Contruimos la matriz pasando los datos a binario\n",
    "\n",
    "x_data_bin = tkn.sequences_to_matrix(x_data, mode='binary')\n",
    "y_data_cat = to_categorical(y_data, num_clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "mVCpasedQepb",
    "outputId": "568305d5-dfb3-4f3b-aa4c-6ff3dac0b70c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: (173, 100) \n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "\n",
      "Test Set: (173, 7)   (usaremos este) \n",
      " [0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Revisamos los como queda la representacion\n",
    "\n",
    "entry = 10\n",
    "print(f'Train Set: {x_data_bin.shape} \\n {x_data_bin[entry]}\\n')\n",
    "print(f'Test Set: {y_data_cat.shape}   (usaremos este) \\n {y_data_cat[entry]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ox3JNVrwQc2o"
   },
   "outputs": [],
   "source": [
    "# Siguiendo la interpretación del paper\n",
    "\n",
    "A_matrix = np.transpose(x_data_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTEMF7ZhSrYl"
   },
   "source": [
    "# NMF Jerárquico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aa4N91wyStwu"
   },
   "source": [
    "* Ya tenemos unos datos preprocesados de forma ideal para nuestro trabajo\n",
    "* Realizaremos la secuencia de NMF jerárquico siguiendo el paper\n",
    "  * [Fast Rank-2 Nonnegative Matrix Factorization for Hierarchical Document Clustering](https://smallk.github.io/papers/hierNMF2.pdf)\n",
    "\n",
    "* Recordar que el objetivo es minimizar la siguiente operación\n",
    "  $$\\min_{W \\geq 0, H \\geq 0} ||A - WH||_2^{2}$$\n",
    "  * A través de la resolución de los subproblemas convexos\n",
    "    $$\\min_{H \\geq 0} ||A - WH||_2^{2}$$\n",
    "    $$\\min_{W \\geq 0} ||A^T - H^T W^T||_2^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NcwIIZe32N7"
   },
   "source": [
    "## NMF Rank-2\n",
    "\n",
    "* Usaremos el algoritmo Rank-2 para generar una estructura de árbol binario jerárquico "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9slMc7UPuZPW"
   },
   "outputs": [],
   "source": [
    "def rank2(A, W):\n",
    "  \"\"\"\n",
    "  Recibe las matrices objetivo A y su matriz izquierda W\n",
    "  Calcula la resolución iterativa de la minimización según el paper\n",
    "  y obtenemos la minimización de H a partir de W\n",
    "  Retorna las matrices W, H de las descomposición\n",
    "  \"\"\"\n",
    "  m, n = np.shape(A)\n",
    "\n",
    "  # resolvemos por minimos cuadrados\n",
    "  H = np.linalg.solve(np.dot(np.transpose(W), W), np.dot(np.transpose(W), A))\n",
    "\n",
    "  # Separamos en columnas\n",
    "  w1, w2 = W[:, 0], W[:, 1]\n",
    "  beta1, beta2 = np.linalg.norm(w1), np.linalg.norm(w2)\n",
    "\n",
    "  # normalizamos\n",
    "  u, v = np.dot(np.transpose(A), w1)/beta1, np.dot(np.transpose(A), w2)/beta2\n",
    "\n",
    "  for j in range(n):\n",
    "    # Para cada vector determinamos si cumple con la solucion\n",
    "    retorno_j = np.zeros(2)\n",
    "    if (H[:, j] >= 0).all():\n",
    "      continue\n",
    "    elif u[j]*beta1 >= v[j]*beta2:\n",
    "      retorno_j[0] = u[j]\n",
    "    else:\n",
    "      retorno_j[1] = v[j]\n",
    "    H[:, j] = retorno_j\n",
    "  return W, H\n",
    "\n",
    "def NMF_rank2(A, W=None, H=None, k=2, **kwargs):\n",
    "  \"\"\"\n",
    "  Recibe la matriz objetivo y matrices iniciales\n",
    "  Se realiza dos veces la minimización primero para H\n",
    "  y luego para W\n",
    "  Retorna la descomposición W, H de baja calidad\n",
    "  \"\"\"\n",
    "  m, n = np.shape(A)\n",
    "\n",
    "  # Iniciamos las matrices\n",
    "  if W is None:\n",
    "    W = np.random.rand(m, k)\n",
    "\n",
    "  if H is None:\n",
    "    H = np.zeros((k, n))\n",
    "  \n",
    "  # Realizamos las minimizaciones\n",
    "  W, H = rank2(A, W)\n",
    "  HT, WT = rank2(np.transpose(A), np.transpose(H))\n",
    "  # Retornamos los valores que resultaron minimizados\n",
    "  return np.transpose(WT), np.transpose(HT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gu4Uze6DOUOW"
   },
   "outputs": [],
   "source": [
    "def calculo_NMF(A, max_iteraciones=15, k=2, W=None, H=None, error=0.5):\n",
    "  \"\"\"\n",
    "  Recibe la matriz objetivo A, dimension k,\n",
    "  máximo de iteraciones y matrices iniciales\n",
    "  Realiza de forma recursiva la aplicación de rank-2\n",
    "  para así obtener una mejor aproximación\n",
    "  Retorna los elementos W, H que aproximan A\n",
    "  al alcanzar una cota de error o superar el maximo\n",
    "  \"\"\"\n",
    "  # Inicializamos las matrices\n",
    "  m, n = np.shape(A)\n",
    "  if W is None:\n",
    "    W = np.random.rand(m, k)\n",
    "\n",
    "  for i in range(max_iteraciones):\n",
    "    W, H = rank2(A, W)\n",
    "    HT, WT = rank2(np.transpose(A), np.transpose(H))\n",
    "    W, H = np.transpose(WT), np.transpose(HT)\n",
    "    if (np.linalg.norm(A - np.dot(W, H))) < error: break\n",
    "  return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2AEFL66POis"
   },
   "outputs": [],
   "source": [
    "def normalizar_descomposicion(W, H):\n",
    "  \"\"\"\n",
    "  Normaliza las columnas de W y pondera respectivamente\n",
    "  las filas de H para el resultado esperado\n",
    "  \"\"\"\n",
    "\n",
    "  for j in range(2):\n",
    "    norma = np.linalg.norm(W[:, j])\n",
    "    W[:, j] = W[:, j]/norma\n",
    "    H[j, :] = H[j, :]*norma\n",
    "  return W, H\n",
    "\n",
    "\n",
    "def calcular_descomposicion(A_matrix, max_iteraciones=15, max_intentos=10):\n",
    "  \"\"\"\n",
    "  Recibe matriz objetivo, cantidad maxima iteraciones e intentos de calcular\n",
    "  Calcula la descomposición rank-2 de forma reiterativa\n",
    "  Si el i-esimo intento alcanza la cota\n",
    "  se retorna la descomposición W, H\n",
    "  \"\"\"\n",
    "  salida, excepcion = False, False\n",
    "  for i in range(max_intentos):\n",
    "    try:\n",
    "      W, H = calculo_NMF(A_matrix, max_iteraciones, k=2, W=None, H=None)\n",
    "      error = np.linalg.norm(np.dot(W, H) - A_matrix)\n",
    "      if error < 60:\n",
    "        salida = True\n",
    "    except:\n",
    "      excepcion = True\n",
    "    else:\n",
    "      if not excepcion and salida:\n",
    "        return normalizar_descomposicion(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_t-3DmOnyv3"
   },
   "source": [
    "### Estructura jerárquica\n",
    "\n",
    "* Para ir generando una estructura jerárquica debemos poder determinar cómo hacer split de los datos\n",
    "\n",
    "  * Necesitamos determinar donde conviene más separar los datos\n",
    "  * Para esto necesitamos una métrica\n",
    "    * Utilizaremos la misma distribución de las palabras que entregan las columnas de la matriz $W$ de la descomposición\n",
    "\n",
    "* Además debemos saber si el split que vamos a hacer conviene, pues deben ser operaciones optimas\n",
    "\n",
    "* El dividir y conquistar los datos nos permitirá realizar el algoritmo de forma recursiva\n",
    "\n",
    "  * Aplicaremos NMF haremos split de los datos\n",
    "  * A estos dos hijos de datos les aplicaremos NMF\n",
    "  * Continuaremos hasta alcanzar cierto objetivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVZ4E3og0I0E"
   },
   "outputs": [],
   "source": [
    "def idx_plbs(arreglo):\n",
    "  \"\"\"\n",
    "  Recibe un arreglo de palabras\n",
    "  Genera un diccionario con los detalles de la palabra\n",
    "  retornando una lista ordenada según relevancia\n",
    "  \"\"\"\n",
    "  largo = len(arreglo)\n",
    "  retorno = list({'word': i, 'value': arreglo[i]} for i in range(largo))\n",
    "  retorno = sorted(retorno, key=lambda x: x['value'], reverse=True)\n",
    "\n",
    "  for i in range(largo):\n",
    "    retorno[i]['id'] = i\n",
    "  return retorno\n",
    "\n",
    "def generar_arrays(array_N, array_L, array_R):\n",
    "  \"\"\"\n",
    "  Recibe los arreglos para poder dividir\n",
    "  Retorna los arreglos ordenados según relevancia de sus palabras\n",
    "  \"\"\"\n",
    "  return idx_plbs(array_N), idx_plbs(array_L), idx_plbs(array_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0iVS2CBb3-YI"
   },
   "outputs": [],
   "source": [
    "def factor_descuento(word, array_L, array_R):\n",
    "  \"\"\"\n",
    "  Recibe los arreglos y la palabra para la cual\n",
    "  se va a calcular su descuento\n",
    "  Retorna el descuento de la palabra\n",
    "  \"\"\"\n",
    "\n",
    "  fi_L = next(x for x in array_L if x['word'] == word)\n",
    "  fi_R = next(x for x in array_R if x['word'] == word)  \n",
    "  return np.log2(len(array_L) - max(fi_L['id'], fi_R['id']) + 1)\n",
    "\n",
    "\n",
    "def ganancia_palabra(word, array_N, array_L, array_R):\n",
    "  \"\"\"\n",
    "  Recibe los arreglos y la palabra de la que se quiere obtener su ganancia\n",
    "  Retorna la ganancia de la palabra\n",
    "  \"\"\"\n",
    "  \n",
    "  descuento = factor_descuento(word, array_L, array_R)\n",
    "  elemento = next(x for x in array_N if x['word'] == word)\n",
    "  return np.log2(len(array_L) - elemento['id'] + 1)/descuento\n",
    "\n",
    "\n",
    "def ganancias(array_N, array_L, array_R):\n",
    "  \"\"\"\n",
    "  Calcula la ganancia del arreglo\n",
    "  Retorna el arreglo de las ganancias y ordenada según ganancia\n",
    "  \"\"\"\n",
    "  retorno = list()\n",
    "  for word in range(len(array_N)):\n",
    "    gan_actual = ganancia_palabra(word, array_N, array_L, array_R)\n",
    "    retorno.append({'palabra': word, 'ganancia': gan_actual})\n",
    "  \n",
    "  return retorno, sorted(retorno, key=lambda x: x['ganancia'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkS5BLqhGJuf"
   },
   "outputs": [],
   "source": [
    "def MDCG(gan_array):\n",
    "  \"\"\"\n",
    "  Calculo de MDCG según el array que se entregue\n",
    "  Retorna el valor de ganancia\n",
    "  \"\"\"\n",
    "  largo = len(gan_array)\n",
    "  elementos = list(gan_array[i]['ganancia']/np.log2(i+1) for i in range(1, largo))\n",
    "  return gan_array[0]['ganancia'] + sum(elementos)\n",
    "\n",
    "\n",
    "def mNDCG(gan_array, gan_sort):\n",
    "  \"\"\"\n",
    "  Calculo del puntaje a través de los arrays listos\n",
    "  \"\"\"\n",
    "  return MDCG(gan_array)/MDCG(gan_sort)\n",
    "\n",
    "\n",
    "def puntaje(f_N, f_L, f_R):\n",
    "  \"\"\"\n",
    "  Calcula el puntaje de la descomposición NMF actual\n",
    "  Retorna el valor que nos ayuda a decidir\n",
    "  \"\"\"\n",
    "  gan, gan_sort = ganancias(*generar_arrays(f_N, f_L, f_R))\n",
    "  return mNDCG(gan, gan_sort)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42-U4DSMNT_b"
   },
   "outputs": [],
   "source": [
    "def elem_puntaje(A_matrix, L_matrix, R_matrix):\n",
    "  \"\"\"\n",
    "  Calcula la descomposición NMF de A (nodo)\n",
    "  y de sus posibles hijos\n",
    "  Retorna los elementos necesarios para determinar si conviene\n",
    "  \"\"\"\n",
    "\n",
    "  condicion = False\n",
    "  while not condicion:\n",
    "    try:\n",
    "      W, H = calcular_descomposicion(A_matrix)\n",
    "      WL, HL = calcular_descomposicion(L_matrix)\n",
    "      WR, HR = calcular_descomposicion(R_matrix)\n",
    "      condicion = True\n",
    "    except:\n",
    "      condicion = False\n",
    "    else:\n",
    "      if condicion:\n",
    "        return W, H, WL, HL, WR, HR\n",
    "\n",
    "def calculo_puntajes(W, H, WL, HL, WR, HR, i):  \n",
    "  \"\"\"\n",
    "  Calcula el puntaje de los hijos del nodo\n",
    "  a partir de los \n",
    "  \"\"\"\n",
    "  X = W[:, i].copy()\n",
    "\n",
    "  puntaje_N1 = puntaje(X, WL[:, 0], WL[:, 1])\n",
    "  puntaje_N2 = puntaje(X, WR[:, 0], WR[:, 1])\n",
    "\n",
    "  return puntaje_N1, puntaje_N2\n",
    "\n",
    "\n",
    "def puntajes_hijos(A, L, R):\n",
    "  \"\"\"\n",
    "  Genera el calculo del puntaje a partir de los\n",
    "  elementos necesario a partir del nodo\n",
    "  \"\"\"\n",
    "  # W, H, WL, HL, WR, HR = elem_puntaje(A, L, R)\n",
    "  return calculo_puntajes(*elem_puntaje(A, L, R), 0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjhkLjQoaOVE"
   },
   "outputs": [],
   "source": [
    "def agregar_columna(A, columna):\n",
    "  \"\"\"\n",
    "  Agrega columna a la matriz A sin importar su contenido\n",
    "  Retorna la matriz con la columna añadida\n",
    "  \"\"\"\n",
    "  if A is None:\n",
    "    A = np.zeros((len(columna), 1))\n",
    "    A[:, 0] = columna\n",
    "  else:\n",
    "    A = np.column_stack((A,columna))\n",
    "  return A\n",
    "\n",
    "\n",
    "def split_matrix(A_matrix, W, H):\n",
    "  \"\"\"\n",
    "  Separación de la matriz por contenido\n",
    "  Retorna la separación en dos matrices\n",
    "  \"\"\"\n",
    "  m, n = np.shape(A_matrix)\n",
    "\n",
    "  A1, A2 = None, None\n",
    "\n",
    "  for j in range(n):\n",
    "    if H[0][j] > H [1][j]:\n",
    "      A1 = agregar_columna(A1, A_matrix[:, j])\n",
    "    else:\n",
    "      A2 = agregar_columna(A2, A_matrix[:, j])\n",
    "\n",
    "  if A1.shape[1] >= A2.shape[1]:\n",
    "    return A1, A2\n",
    "  return A2, A1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhzFR8xYf5fq"
   },
   "source": [
    "# Parte final\n",
    "\n",
    "Vamos a generar un arreglo que contenga la estructura de nuestro arbol\n",
    "* Será un ejemplo sencillo por lo que usaremos pocos nodos\n",
    "* Usamos un arreglo para los nodos generado\n",
    "* Retornaría este arreglo que describe la estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fbk-HUhf4-I"
   },
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "\n",
    "numero_nodos = 7 # cantidad nodos para crear\n",
    "beta = 1.1 # diferencia de tamaño mínima que habrá entre los nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJgG4CuvgyXS"
   },
   "outputs": [],
   "source": [
    "def seleccionar_nodo(lista_nodos):\n",
    "  \"\"\"\n",
    "  Recibe el arreglo de los nodos de la estructura\n",
    "  Calcula cual nodo es conveniente separar y lo retorna\n",
    "  \"\"\"\n",
    "  if len(lista_nodos) == 1:\n",
    "    return lista_nodos[0]\n",
    "  lista_nodos = sorted(lista_nodos,\n",
    "                       key = lambda i: i['puntaje'],\n",
    "                       reverse=True)\n",
    "  return lista_nodos[1]\n",
    "\n",
    "def menor_puntaje(lista_nodos, puntaje_N2):\n",
    "  \"\"\"\n",
    "  Determina si el puntaje actual es el\n",
    "  menor comparando con todos los nodos\n",
    "  Retorna bool si conviene hacer split a ese nodo\n",
    "  \"\"\"\n",
    "  for elemento in lista_nodos:\n",
    "    if elemento['puntaje'] <= puntaje_N2:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdxgV1Pf1S3p"
   },
   "outputs": [],
   "source": [
    "def arreglo_palabras(W, i):\n",
    "  \"\"\"\n",
    "  Genera el arreglo de las palabras de la columna i\n",
    "  Retorna arreglo diccionarios con los datos ordenados\n",
    "  \"\"\"\n",
    "\n",
    "  entradas = ['idx', 'value']\n",
    "  distribucion, retorno = W[:, i], list()\n",
    "  for item in enumerate(distribucion):\n",
    "    retorno.append(dict(zip(entradas, item)))\n",
    "  return sorted(retorno, key=lambda i: i['value'], reverse=True)\n",
    "\n",
    "\n",
    "def encontrar_significado(arreglo):\n",
    "  \"\"\"\n",
    "  Recibe el arreglo de indices de palabras\n",
    "  Retorna los elementos con atributo word que es el significado\n",
    "  \"\"\"\n",
    "  for i in range(len(arreglo)):\n",
    "    arreglo[i]['word'] = index_to_word[arreglo[i]['idx'] + 4]\n",
    "  return arreglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ay6OAWxusg3o"
   },
   "outputs": [],
   "source": [
    "def arreglo_palabras(W, i):\n",
    "  entradas = ['idx', 'value']\n",
    "  distribucion = W[:, i]\n",
    "  retorno = list()\n",
    "  for item in enumerate(distribucion):\n",
    "    retorno.append(dict(zip(entradas, item)))\n",
    "  retorno = sorted(retorno, key = lambda i: i['value'], reverse=True)\n",
    "  return retorno\n",
    "\n",
    "\n",
    "def encontrar_significado(arreglo):\n",
    "  # Como teniamos ordenada la representacion\n",
    "  # Usamos nuestro index to words para encontrar la palabra\n",
    "\n",
    "  for i in range(len(arreglo)):\n",
    "    arreglo[i]['word'] = index_to_word[arreglo[i]['idx'] + 4]\n",
    "  return arreglo\n",
    "\n",
    "\n",
    "def palabras_destacadas(W, cantidad=3):\n",
    "  n = int(np.round(cantidad/2))+1\n",
    "  arreglo_1 = encontrar_significado(arreglo_palabras(W, 0)[:n])\n",
    "  arreglo_2 = encontrar_significado(arreglo_palabras(W, 1)[:n])\n",
    "  arreglo_1.extend(arreglo_2)\n",
    "  return arreglo_1\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQzkWiA8f2yg"
   },
   "outputs": [],
   "source": [
    "def jerarquizacion(A_matrix):\n",
    "  \"\"\"\n",
    "  Genera la estructura de jerarquía realizando\n",
    "  descomposiciones NMF de forma recursiva\n",
    "  Retorna la estructura \n",
    "  \"\"\"\n",
    "\n",
    "  outliner = None\n",
    "  lista_nodos = list()\n",
    "\n",
    "  primer_nodo = {\n",
    "    'id': 0,\n",
    "    'parent': None,\n",
    "    'matrix': A_matrix,\n",
    "    'puntaje': 1,\n",
    "    'shape': A_matrix.shape,\n",
    "  }\n",
    "\n",
    "  lista_nodos.append(primer_nodo)\n",
    "\n",
    "  for i in range(1, numero_nodos, 2):\n",
    "    M = seleccionar_nodo(lista_nodos)\n",
    "    W, H = calcular_descomposicion(M['matrix'])\n",
    "    M['W'], M['H'] = W, H\n",
    "    N1, N2 = split_matrix(M['matrix'], W, H)\n",
    "    puntaje_N1, puntaje_N2 = puntajes_hijos(M['matrix'], N1, N2)\n",
    "    \n",
    "    N1_nodo = {\n",
    "    'id': i+1,\n",
    "    'parent': M['id'],\n",
    "    'matrix': N1,\n",
    "    'puntaje': puntaje_N1,\n",
    "    'shape': N1.shape,\n",
    "    'hijos': None\n",
    "    }\n",
    "\n",
    "    N2_nodo = {\n",
    "    'id': i+2,\n",
    "    'parent': M['id'],\n",
    "    'matrix': N2,\n",
    "    'puntaje': puntaje_N2,\n",
    "    'shape': N2.shape,\n",
    "    'hijos': None\n",
    "    }\n",
    "\n",
    "    M['hijos'] = [i+1, i+2, ]\n",
    "\n",
    "    lista_nodos.append(N1_nodo)\n",
    "    lista_nodos.append(N2_nodo)\n",
    "\n",
    "  return lista_nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "IPf2qvqkVBtc",
    "outputId": "75ee4779-3c94-44ca-9517-42be9ffb01e2"
   },
   "outputs": [],
   "source": [
    "def matriz_W_lista(lista_nodos):\n",
    "  for nodo in lista_nodos:\n",
    "    if 'W' not in nodo.keys():\n",
    "      nodo['W'], nodo['H'] = calcular_descomposicion(nodo['matrix'])\n",
    "  return lista_nodos\n",
    "\n",
    "\n",
    "def limpiar_lista(lista_nodos):\n",
    "  elementos = ['matrix', 'W', 'H']\n",
    "  new_list = [{k: v for k, v in d.items() if k not in elementos} for d in lista_nodos]\n",
    "  return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_arbol = jerarquizacion(A_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUj8shzZ9B9J"
   },
   "outputs": [],
   "source": [
    "for nodo in lista_arbol:\n",
    "  if 'W' not in nodo.keys():\n",
    "    nodo['W'], nodo['H'] = calcular_descomposicion(nodo['matrix'])\n",
    "    \n",
    "  retorno = palabras_destacadas(nodo['W'], cantidad=10)\n",
    "  #for palabra in retorno:\n",
    "  #  print(palabra['word'])\n",
    "  #print(\"\")\n",
    "  nodo['destacadas'] = retorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OiuTMnkiOfR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'parent': None,\n",
       "  'puntaje': 1,\n",
       "  'shape': (100, 173),\n",
       "  'hijos': [2, 3],\n",
       "  'destacadas': [{'idx': 14, 'value': 0.37536499106798366, 'word': '000'},\n",
       "   {'idx': 53, 'value': 0.3471975061129189, 'word': 'have'},\n",
       "   {'idx': 26, 'value': 0.3467574791463768, 'word': 'be'},\n",
       "   {'idx': 39, 'value': 0.3270400970668038, 'word': 'as'},\n",
       "   {'idx': 11, 'value': 0.2806841094608583, 'word': 'dlrs'},\n",
       "   {'idx': 19, 'value': 0.2685838779716942, 'word': 'is'},\n",
       "   {'idx': 4, 'value': 0.26769512649315785, 'word': 'said'},\n",
       "   {'idx': 6, 'value': 0.2669035815617256, 'word': 'a'},\n",
       "   {'idx': 8, 'value': 0.26416458915879476, 'word': '3'},\n",
       "   {'idx': 10, 'value': 0.2557990964676358, 'word': 'vs'},\n",
       "   {'idx': 5, 'value': 0.24497288891977692, 'word': 'and'},\n",
       "   {'idx': 9, 'value': 0.23906860895636386, 'word': 'for'}]},\n",
       " {'id': 2,\n",
       "  'parent': 0,\n",
       "  'puntaje': 0.49908531073766027,\n",
       "  'shape': (100, 99),\n",
       "  'hijos': [4, 5],\n",
       "  'destacadas': [{'idx': 27, 'value': 0.2388270897606621, 'word': 'with'},\n",
       "   {'idx': 21, 'value': 0.23481044060305567, 'word': 'its'},\n",
       "   {'idx': 42, 'value': 0.23064279323803988, 'word': 'loss'},\n",
       "   {'idx': 24, 'value': 0.2124016214138852, 'word': 'at'},\n",
       "   {'idx': 22, 'value': 0.2012256620125206, 'word': 'cts'},\n",
       "   {'idx': 43, 'value': 0.1763112986237411, 'word': '4'},\n",
       "   {'idx': 8, 'value': 0.3292223118931446, 'word': '3'},\n",
       "   {'idx': 6, 'value': 0.3213583473356591, 'word': 'a'},\n",
       "   {'idx': 4, 'value': 0.3212078970072791, 'word': 'said'},\n",
       "   {'idx': 5, 'value': 0.2948089171726394, 'word': 'and'},\n",
       "   {'idx': 9, 'value': 0.2791217828174776, 'word': 'for'},\n",
       "   {'idx': 7, 'value': 0.24605293577284765, 'word': 'mln'}]},\n",
       " {'id': 3,\n",
       "  'parent': 0,\n",
       "  'puntaje': 0.4268668401415102,\n",
       "  'shape': (100, 74),\n",
       "  'hijos': None,\n",
       "  'destacadas': [{'idx': 43, 'value': 0.4942780648815193, 'word': '4'},\n",
       "   {'idx': 45, 'value': 0.48704012555336024, 'word': 'company'},\n",
       "   {'idx': 60, 'value': 0.48255202017686594, 'word': 'had'},\n",
       "   {'idx': 14, 'value': 0.15492888105591063, 'word': '000'},\n",
       "   {'idx': 53, 'value': 0.13733553717654237, 'word': 'have'},\n",
       "   {'idx': 11, 'value': 0.13716438408052295, 'word': 'dlrs'},\n",
       "   {'idx': 14, 'value': 0.37382431391608306, 'word': '000'},\n",
       "   {'idx': 26, 'value': 0.35190721200335595, 'word': 'be'},\n",
       "   {'idx': 39, 'value': 0.3184465956832527, 'word': 'as'},\n",
       "   {'idx': 53, 'value': 0.3173027873195139, 'word': 'have'},\n",
       "   {'idx': 19, 'value': 0.2684488820735706, 'word': 'is'},\n",
       "   {'idx': 11, 'value': 0.24625511806954434, 'word': 'dlrs'}]},\n",
       " {'id': 4,\n",
       "  'parent': 2,\n",
       "  'puntaje': 0.5641075896716296,\n",
       "  'shape': (100, 69),\n",
       "  'hijos': None,\n",
       "  'destacadas': [{'idx': 10, 'value': 0.33462156065591075, 'word': 'vs'},\n",
       "   {'idx': 6, 'value': 0.3096266902232455, 'word': 'a'},\n",
       "   {'idx': 8, 'value': 0.2857961820778334, 'word': '3'},\n",
       "   {'idx': 5, 'value': 0.2755384867130662, 'word': 'and'},\n",
       "   {'idx': 49, 'value': 0.2568087022766454, 'word': 'shr'},\n",
       "   {'idx': 4, 'value': 0.2540948413252333, 'word': 'said'},\n",
       "   {'idx': 48, 'value': 0.31227984352229304, 'word': 'this'},\n",
       "   {'idx': 33, 'value': 0.30964516447299817, 'word': 'u'},\n",
       "   {'idx': 56, 'value': 0.26363757434537505, 'word': 'bank'},\n",
       "   {'idx': 79, 'value': 0.2604565825198946, 'word': '10'},\n",
       "   {'idx': 42, 'value': 0.2565373407656391, 'word': 'loss'},\n",
       "   {'idx': 82, 'value': 0.23647420911232608, 'word': 'revs'}]},\n",
       " {'id': 5,\n",
       "  'parent': 2,\n",
       "  'puntaje': 0.7173054199413685,\n",
       "  'shape': (100, 30),\n",
       "  'hijos': [6, 7],\n",
       "  'destacadas': [{'idx': 24, 'value': 0.240684777333432, 'word': 'at'},\n",
       "   {'idx': 30, 'value': 0.23340776987346185, 'word': 'was'},\n",
       "   {'idx': 23, 'value': 0.22076575111789165, 'word': 'by'},\n",
       "   {'idx': 34, 'value': 0.21279797021908042, 'word': 's'},\n",
       "   {'idx': 43, 'value': 0.21134564838434527, 'word': '4'},\n",
       "   {'idx': 25, 'value': 0.20182432738277017, 'word': 'year'},\n",
       "   {'idx': 8, 'value': 0.2836640671063266, 'word': '3'},\n",
       "   {'idx': 16, 'value': 0.28139174550127427, 'word': 'pct'},\n",
       "   {'idx': 4, 'value': 0.264730412299418, 'word': 'said'},\n",
       "   {'idx': 6, 'value': 0.264730412299418, 'word': 'a'},\n",
       "   {'idx': 10, 'value': 0.264730412299418, 'word': 'vs'},\n",
       "   {'idx': 42, 'value': 0.26243224739287047, 'word': 'loss'}]},\n",
       " {'id': 6,\n",
       "  'parent': 5,\n",
       "  'puntaje': 0.45374533362967195,\n",
       "  'shape': (100, 22),\n",
       "  'hijos': None,\n",
       "  'destacadas': [{'idx': 18, 'value': 0.27271730899262825, 'word': 'from'},\n",
       "   {'idx': 65, 'value': 0.20817587886036898, 'word': 'about'},\n",
       "   {'idx': 4, 'value': 0.1960882954209511, 'word': 'said'},\n",
       "   {'idx': 6, 'value': 0.1960882954209511, 'word': 'a'},\n",
       "   {'idx': 10, 'value': 0.1960882954209511, 'word': 'vs'},\n",
       "   {'idx': 8, 'value': 0.19559588131652794, 'word': '3'},\n",
       "   {'idx': 98, 'value': 0.7198201834833757, 'word': '15'},\n",
       "   {'idx': 83, 'value': 0.25930813108622647, 'word': 'prices'},\n",
       "   {'idx': 88, 'value': 0.25930813108622647, 'word': 'after'},\n",
       "   {'idx': 8, 'value': 0.1296073094943327, 'word': '3'},\n",
       "   {'idx': 4, 'value': 0.12238356231474277, 'word': 'said'},\n",
       "   {'idx': 6, 'value': 0.12238356231474277, 'word': 'a'}]},\n",
       " {'id': 7,\n",
       "  'parent': 5,\n",
       "  'puntaje': 0.4818943369654686,\n",
       "  'shape': (100, 8),\n",
       "  'hijos': None,\n",
       "  'destacadas': [{'idx': 19, 'value': 0.38228437469946935, 'word': 'is'},\n",
       "   {'idx': 94, 'value': 0.38228437469946935, 'word': 'quarter'},\n",
       "   {'idx': 18, 'value': 0.34045274063721886, 'word': 'from'},\n",
       "   {'idx': 32, 'value': 0.34045274063721886, 'word': 'he'},\n",
       "   {'idx': 60, 'value': 0.34045274063721886, 'word': 'had'},\n",
       "   {'idx': 89, 'value': 0.34045274063721886, 'word': 'april'},\n",
       "   {'idx': 68, 'value': 0.29093960005110325, 'word': '9'},\n",
       "   {'idx': 20, 'value': 0.24305809784824012, 'word': 'that'},\n",
       "   {'idx': 45, 'value': 0.19545414963270452, 'word': 'company'},\n",
       "   {'idx': 4, 'value': 0.18035886945902732, 'word': 'said'},\n",
       "   {'idx': 5, 'value': 0.18035886945902732, 'word': 'and'},\n",
       "   {'idx': 6, 'value': 0.18035886945902732, 'word': 'a'}]}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limpiar_lista(lista_arbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpP909znWOi4"
   },
   "source": [
    "## TO DO\n",
    "\n",
    "* Visualización del arbol en Observable\n",
    "\n",
    "* Uso dataset Andrés Carvallo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6HMGSCyKq6r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Reuters.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
