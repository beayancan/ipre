{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publimed estilo Reuters\n",
    "\n",
    "En el siguiente notebook vamos a utilizar el dataset publimed con el cual aplicaremos NMF, por lo que:\n",
    "* El dataset corresponde a una recopilación de preguntas médicas sobre documentos de diversos tópicos médicos\n",
    "* Vamos a pasar el dataset al estilo Reuters\n",
    "    * Recordar que esto se refiere a una representacion a través de las palabras y frecuencias\n",
    "* Utilzaremos sobre ella la descomposición NMF Rank-2\n",
    "    * Nos enfocaremos en una clasificación sobre los tópicos de los documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargo y manejo de los datos\n",
    "\n",
    "* Convertiremos el dataset publimed que mantenemos almacenados en un documento CSV\n",
    "* Leeremos los documentos y rescataremos los datos que nos interesan\n",
    "* Codificaremos tanto su contenido como su etiqueta para dejarlo en formato \"Reuters\"\n",
    "    * Esta se refiere a una representacion a través de las palabras del doc\n",
    "    * La codificacion depende de la frecuencia de las palabras del vocabulario utilizado\n",
    "* Este preceso busca poder convertir cualquier dataset a modo reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar vocabulario\n",
    "\n",
    "* Contaremos las palabras para generar el vocabulario que nos permitirán establecer la frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(url):\n",
    "  \"\"\"\n",
    "  Cargamos los datos del dataset publimed\n",
    "  Retornamos un dataframe con los datos\n",
    "  \"\"\"\n",
    "  return pd.read_csv(url, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puntuacion(text):\n",
    "  \"\"\"\n",
    "  Recibe un texto y elimina los signos de puntuacion\n",
    "  \"\"\"\n",
    "  forbidden = (\"?\", \"¿\", \"¡\", \"!\", \",\", \".\", \";\", \":\", \"-\", \"[\", \"]\", \"'\", \"(\", \")\", \"{\", \"}\", '\"', \"/\")\n",
    "  texto = text.lower()\n",
    "  aux = \"\" \n",
    "  for v in texto:\n",
    "    if not v in forbidden:\n",
    "      aux += v\n",
    "  aux = aux.replace(\"  \", \" \")\n",
    "  return aux\n",
    "\n",
    "def solo_oraciones(data, atributo=\"title\"):\n",
    "  \"\"\"\n",
    "  Obtiene una lista de las oraciones\n",
    "  de los atributos seleccionados de data\n",
    "  \"\"\"\n",
    "  return list(puntuacion(x) for x in set(data[atributo]) if type(x) == str)\n",
    "\n",
    "def todas_oraciones(data, atributos=['title', 'abstract']):\n",
    "  \"\"\"\n",
    "  Retorna todas las oraciones de la data\n",
    "  que pertenecen a los atributos seleccioandos\n",
    "  \"\"\"\n",
    "  return reduce(lambda x,y: x + solo_oraciones(data, y), atributos, list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(counts, string):\n",
    "  \"\"\"\n",
    "  Recibe un registro de los conteos de las palabras\n",
    "  y un string.\n",
    "  Actualiza el contador de las palabras y lo retorna\n",
    "  \"\"\"\n",
    "  words = string.split()\n",
    "  for word in words:\n",
    "    if word in counts:\n",
    "      counts[word] += 1\n",
    "    else:\n",
    "      counts[word] = 1\n",
    "  return counts\n",
    "\n",
    "\n",
    "def obtener_palabras(data, atributos=['title', 'abstract']):\n",
    "  \"\"\"\n",
    "  Recibe el data completo\n",
    "  Retorna un arreglo de todas las palabras\n",
    "  de los atributos que se deseen\n",
    "  \"\"\"\n",
    "  oraciones, palabras = todas_oraciones(data, atributos), dict()\n",
    "  oraciones = list(set(oraciones))\n",
    "  for frase in oraciones:\n",
    "    palabras = word_count(palabras, frase)\n",
    "  return palabras\n",
    "\n",
    "\n",
    "def construir_vocabulario(palabras):\n",
    "  \"\"\"\n",
    "  Cuenta la cantidad de veces que son utilizadas las palabras\n",
    "  retorna un arreglo de las palabras con su frecuencia\n",
    "  \"\"\"\n",
    "  keys = ['word', 'frequency']\n",
    "  vocabulario = list(dict(zip(keys, tupla)) for tupla in palabras.items())\n",
    "  return sorted(vocabulario, key = lambda i: i['frequency'], reverse=True)\n",
    "\n",
    "\n",
    "def asignar_id(vocabulario):\n",
    "  \"\"\"\n",
    "  Agrega un id para las palabras utilizadas\n",
    "  \"\"\"\n",
    "  for idx, palabra in enumerate(vocabulario):\n",
    "    palabra['id'] = idx + 3\n",
    "  return vocabulario\n",
    "\n",
    "\n",
    "def generar_vocabulario(data):\n",
    "  \"\"\"\n",
    "  A partir de la data determina las palabras y genera el vocabulario\n",
    "  para realizar la configuracion de los datos\n",
    "  \"\"\"\n",
    "  palabras = obtener_palabras(data)\n",
    "  vocabulario = construir_vocabulario(palabras)\n",
    "  return asignar_id(vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar codificiacion\n",
    "\n",
    "* A partir del vocabulario generado que incluye la frecuencia de uso de las palabras\n",
    "* Generamos la configuracion de los datos utilizados\n",
    "* Esto nos permitirá obtener una representacion de cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_labels(data):\n",
    "  registro, topicos, keys = dict(), list(), ['topic', 'frequency']\n",
    "\n",
    "  for index, row in data.iterrows():\n",
    "    topico = row['topic_name']\n",
    "    if topico in registro:\n",
    "      registro[topico] += 1\n",
    "    else:\n",
    "      registro[topico] = 1\n",
    "\n",
    "  for tupla in registro.items():\n",
    "    tupla_actual = dict(zip(keys, tupla))\n",
    "    topicos.append(tupla_actual)\n",
    "\n",
    "  topicos = sorted(topicos, key = lambda i: i['frequency'], reverse=True)\n",
    "\n",
    "  retorno = dict()\n",
    "  for idx, topico in enumerate(topicos):\n",
    "    topico['id_label'] = idx+1\n",
    "    retorno[topico['topic']] = idx+1\n",
    "    #print(topico['topic'])\n",
    "  return topicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_topic(nombre, label_topic):\n",
    "  for idx, elemento in enumerate(label_topic):\n",
    "    if nombre == elemento['topic']:\n",
    "      return idx+1\n",
    "\n",
    "def id_topicos(data, label_topic):\n",
    "    return list( encontrar_topic(row['topic_name'], label_topic) for _, row in data.iterrows() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unir_contenido(row):\n",
    "  if row['code_abstract'] is None:\n",
    "    return row['code_title']\n",
    "  return row['code_title'] + row['code_abstract']\n",
    "\n",
    "def generar_contenido(data):\n",
    "  return list(unir_contenido(row) for _, row in data.iterrows())\n",
    "\n",
    "def generar_dataset(data):\n",
    "  retorno = dict()\n",
    "  label_topic = generar_labels(data)\n",
    "  retorno['content'] = generar_contenido(data)\n",
    "  retorno['label'] = id_topicos(data, label_topic)\n",
    "  return retorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de variables para dataset\n",
    "\n",
    "* Generamos opciones para poder manejar el dataset y su separacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtro_ranking(elemento, num_words, skip_top):\n",
    "  retorno = list()\n",
    "  maximo = num_words + skip_top\n",
    "  for x in elemento:\n",
    "    if skip_top < x < maximo:\n",
    "      retorno.append(x)\n",
    "  return retorno\n",
    "\n",
    "def filtros(content, max_len, num_words, skip_top):\n",
    "  retorno = list()\n",
    "  for elemento in content:\n",
    "    if num_words and elemento:\n",
    "      elemento = filtro_ranking(elemento, num_words, skip_top)\n",
    "    if max_len and elemento:\n",
    "      elemento = elemento[:max_len]\n",
    "    retorno.append(elemento)\n",
    "  return retorno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_id(vocabulario, palabra, atributo, idx):\n",
    "  \"\"\"\n",
    "  Encontramos el id que posee la palabra en el vocabulario\n",
    "  \"\"\"\n",
    "  for tupla in vocabulario:\n",
    "    if tupla['word'] == palabra:\n",
    "      return(tupla['id'])\n",
    "  return None\n",
    "\n",
    "def to_code(vocabulario, oracion, atributo, idx):\n",
    "  \"\"\"\n",
    "  Codificamos las frases utilizadas para el atributo\n",
    "  \"\"\"\n",
    "  oracion = oracion.strip().split()\n",
    "  return list(encontrar_id(vocabulario, palabra, atributo, idx) for palabra in oracion)\n",
    "\n",
    "def code_atributo(data, vocabulario, atributo):\n",
    "  \"\"\"\n",
    "  Codifica el atributo a partir de la representacion\n",
    "  a partir de la frecuencia suada en ese atributo\n",
    "  \"\"\"\n",
    "  columna_atributo = list()\n",
    "  for index, row in data.iterrows():\n",
    "    if type(row[atributo]) is str:\n",
    "      auxiliar = to_code(vocabulario, puntuacion(row[atributo]), atributo, index)\n",
    "    else:\n",
    "      row[atributo], auxiliar = None, None\n",
    "    columna_atributo.append(auxiliar)\n",
    "  data[f'code_{atributo}'] = columna_atributo\n",
    "  return data\n",
    "\n",
    "def code_atributos(data, vocabulario, atributos=['title', 'abstract']):\n",
    "  \"\"\"\n",
    "  Codifica los atributos de la data para\n",
    "  una representacion de frecuencia de palabras\n",
    "  \"\"\"\n",
    "  for atributo in atributos:\n",
    "    data = code_atributo(data, vocabulario, atributo)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, vocabulario, test_split, max_len=None, num_words=None, skip_top=None):\n",
    "  #data = cargar_datos(url)\n",
    "  #vocabulario = generar_vocabulario(data)\n",
    "  data_procesada = code_atributos(data, vocabulario)\n",
    "  dataset = generar_dataset(data_procesada)\n",
    "  largo = len(dataset['content'])\n",
    "  len_train = int(largo*test_split)\n",
    "  #len_test = largo - len_train\n",
    "  \n",
    "  #print(largo, len_train, test_split)\n",
    "  \n",
    "  dataset['content'] = filtros(dataset['content'], max_len, num_words, skip_top)\n",
    "  \n",
    "  train = np.array(dataset['content'][len_train:]), np.array(dataset['label'][len_train:])\n",
    "  test = np.array(dataset['content'][:len_train]), np.array(dataset['label'][:len_train])\n",
    "  #print(len(train), len(test))\n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edición del dataset\n",
    "\n",
    "* Generamos funciones para poder representar mejor el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_repetidos(x_train, y_train):\n",
    "  x_retorno, y_retorno = list(), list()\n",
    "  \n",
    "  for i, elemento in enumerate(x_train):\n",
    "    if x_retorno and elemento == x_retorno[-1]:\n",
    "      y_retorno[-1].append(y_train[i])\n",
    "      continue\n",
    "    x_retorno.append(elemento)\n",
    "    y_retorno.append([y_train[i], ])\n",
    "  return x_retorno, y_retorno\n",
    "\n",
    "def mensaje_code(vocabulario, code):\n",
    "  retorno = list( to_word(x, vocabulario) for x in code)\n",
    "  return \" \".join(retorno)\n",
    "\n",
    "def to_word(idx, vocabulario):\n",
    "  return vocabulario[idx-3]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_vacios(x_train, y_train):\n",
    "  \"\"\"\n",
    "  Por diferentes opciones pueden quedar valores vacios\n",
    "  Eliminamos esos valores vacios\n",
    "  \"\"\"\n",
    "  x_retorno, y_retorno = list(), list()\n",
    "  for i, x in enumerate(x_train):\n",
    "    if x:\n",
    "      x_retorno.append(x)\n",
    "      y_retorno.append(y_train[i])\n",
    "  return x_retorno, y_retorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carguemos los datos\n",
    "\n",
    "* Vamos a cargar los datos según caracteristicas que hagan más fácil de entender el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETROS\n",
    "\n",
    "num_words = 100 # solo tomaremos las 100 primeras palabras\n",
    "skip_top = 15 # palabras que no consideraremos\n",
    "max_len = None # N° de palabras vocabulario\n",
    "test_split = 0.05 # Porcentaje de palabras en el test set\n",
    "url = '../dataset/publimed_medium.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_labels(labels):\n",
    "  return list(x['topic'] for x in labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cargar_datos(url)\n",
    "vocabulario = generar_vocabulario(data)\n",
    "label_topic = mapping_labels(generar_labels(data))\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data(data,\n",
    "                                                 vocabulario,\n",
    "                                                 test_split=test_split,\n",
    "                                                 max_len=max_len,\n",
    "                                                 num_words=num_words,\n",
    "                                                 skip_top=skip_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño train set: (3325,), (3325,)\n",
      "Tamaño test set: (175,), (175,)\n",
      "Cantidad clases: 9\n"
     ]
    }
   ],
   "source": [
    "print(f'Tamaño train set: {x_train.shape}, {y_train.shape}')\n",
    "print(f'Tamaño test set: {x_test.shape}, {y_test.shape}')\n",
    "print(f'Cantidad clases: {max(max(y_train), max(y_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = eliminar_vacios(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clases = max(y_train)\n",
    "n_clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1 - freq: 1152\n",
      "id: 2 - freq: 1146\n",
      "id: 3 - freq: 1144\n",
      "id: 4 - freq: 53\n",
      "id: 5 - freq: 1\n",
      "id: 6 - freq: 1\n",
      "id: 7 - freq: 1\n",
      "id: 8 - freq: 1\n",
      "id: 9 - freq: 1\n"
     ]
    }
   ],
   "source": [
    "for label in generar_labels(data):\n",
    "  print(f\"id: {label['id_label']} - freq: {label['frequency']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicamos Rank-2\n",
    "\n",
    "Ya que tenemos el dataset listo para poder trabajar en estilo \"Reuters\"\n",
    "\n",
    "* Vamos a importar las funciones del tutorial del uso de reuters\n",
    "* Utilizamos las funciones para poder separar en ciertos grupos\n",
    "\n",
    "### Importamos las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.reuters_functions as reu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conteo_labels(n, p):\n",
    "  \"\"\"\n",
    "  Para una mejor representacion de los labels\n",
    "  utilizaremos medidores para que no supere ciertas barreras\n",
    "  \"\"\"\n",
    "  return list( [0, randrange(p-5, p+5)] for _ in range(n) )\n",
    "\n",
    "\n",
    "def reducir_labels(data_array, labels, k=7, pivote=25):\n",
    "  \"\"\"\n",
    "  Filtramos los documentos que pertenezcan a las primeras k clases\n",
    "  Retornamos el arreglo con los documentos y sus correspondientes labels\n",
    "  \"\"\"\n",
    "  \n",
    "  conteo = conteo_labels(max(labels), pivote)\n",
    "  retorno, retorno_labels = list(), list()\n",
    "  for i in range(len(data_array)):\n",
    "    if labels[i] < k and conteo[labels[i]][0] < conteo[labels[i]][1]:\n",
    "      retorno.append(data_array[i])\n",
    "      retorno_labels.append(labels[i])\n",
    "      conteo[labels[i]][0] += 1\n",
    "  return np.array(retorno), retorno_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = reducir_labels(x_train, y_train, k=n_clases, pivote=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasar los datos a matriz doc-term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos tokenizer para producirlo\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tkn = Tokenizer(num_words=num_words) # tamaño del vocabulario\n",
    "num_clases = max(y_data) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruimos la matriz pasando los datos a binario\n",
    "\n",
    "x_data_bin = tkn.sequences_to_matrix(x_data, mode='binary')\n",
    "y_data_cat = to_categorical(y_data, num_clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set \n",
      "\n",
      "  Values: (106, 100)    n° docs x n° words \n",
      "  Ej: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "  Labels: (106, 9)   (usaremos este) \n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Revisamos los como queda la representacion\n",
    "\n",
    "entry = 10\n",
    "print(f'Train Set \\n\\n  Values: {x_data_bin.shape}    n° docs x n° words \\n  Ej: {x_data_bin[entry]}\\n')\n",
    "print(f'\\n  Labels: {y_data_cat.shape}   (usaremos este) \\n {y_data_cat[entry]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siguiendo la interpretación del paper\n",
    "\n",
    "A_matrix = np.transpose(x_data_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_palabras_destacadas(lista_arbol, index_to_word, cantidad=10):\n",
    "  for nodo in lista_arbol:\n",
    "    if nodo['matrix'].shape[1] == 1:\n",
    "      nodo['W'] = nodo['matrix']\n",
    "    elif 'W' not in nodo.keys():\n",
    "      nodo['W'], nodo['H'] = reu.calcular_descomposicion(nodo['matrix'])\n",
    "    nodo['destacadas'] = reu.palabras_destacadas(nodo['W'], index_to_word, cantidad)\n",
    "  return lista_arbol\n",
    "\n",
    "\n",
    "def topicos_relevantes(y_data, columnas, mapping):\n",
    "  \"\"\"\n",
    "  Ordena los topicos relevantes para cada nodo\n",
    "  del arbol jerarquico\n",
    "  \"\"\"\n",
    "  counts = dict()\n",
    "  for col in columnas:\n",
    "    if str(y_data[col]) in counts:\n",
    "      counts[str(y_data[col])] += 1\n",
    "    else:\n",
    "      counts[str(y_data[col])] = 1\n",
    "      \n",
    "  keys = ['label', 'frequency']\n",
    "\n",
    "  auxiliar = list(dict(zip(keys, tupla)) for tupla in counts.items())\n",
    "\n",
    "  labels = sorted(auxiliar, key = lambda i: i['frequency'], reverse=True)\n",
    "  \n",
    "  for elemento in labels:\n",
    "    elemento['label'] = int(elemento['label'])\n",
    "    elemento['label_name'] = mapping[elemento['label']-1]\n",
    "  return labels\n",
    "\n",
    "def palabras_relevantes(palabras):\n",
    "  \"\"\"\n",
    "  Selecciona las palabras más relevantes para cada nodo\n",
    "  del arbol jerarquico\n",
    "  \"\"\"\n",
    "  return list(x['word']['word'] for x in palabras[:5])\n",
    "\n",
    "\n",
    "def presentar_nodos(lista_arbol, y_data, mapping):\n",
    "  \"\"\"\n",
    "  Imprime una vision preeliminar de los nodos del arbol jerarquico\n",
    "  \"\"\"\n",
    "  for nodo in lista_arbol:\n",
    "    print(f\"Nodo {nodo['id']}\")\n",
    "    print(f\"  parent: {nodo['parent']} - leafs {nodo['hijos']}\\n\")\n",
    "    topicos = topicos_relevantes(y_data, nodo['columnas'], mapping)\n",
    "    for topico in topicos:\n",
    "      print(f\"label: {topico['label']}, frecuencia: {topico['frequency']}\")\n",
    "      print(f\"  {topico['label_name']}\")\n",
    "    print(\"\")\n",
    "    palabras = palabras_relevantes(nodo['destacadas'])\n",
    "    frase = \"\"\n",
    "    for w in palabras:\n",
    "      if frase:\n",
    "        frase += \" / \"\n",
    "      frase += w\n",
    "    print(frase, \"\\n----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación NMF Rank-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "\n",
    "numero_nodos = 4 # cantidad nodos para crear\n",
    "beta = 1.1 # diferencia de tamaño mínima que habrá entre los nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = Tokenizer(num_words=num_words) # tamaño del vocabulario\n",
    "num_clases = max(y_data) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_arbol = reu.jerarquizacion(A_matrix, vocabulario, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_arbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_arbol = obtener_palabras_destacadas(lista_arbol, vocabulario, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_arbol = reu.limpiar_lista(lista_arbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#presentar_nodos(lista_arbol, y_data, label_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def top_words(palabras):\n",
    "  arreglo_palabras = dict((x['word']['word'], x['word']['frequency']) for x in palabras)\n",
    "  return dict(sorted(arreglo_palabras.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "def arreglar_destacadas(lista_nodos):\n",
    "  for nodo in lista_nodos:\n",
    "    nodo['destacadas'] = top_words(nodo['destacadas'])\n",
    "  return lista_nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_arbol = arreglar_destacadas(lista_arbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'id': 2,\n",
    "  'parent': 0,\n",
    "  'matrix': None,\n",
    "  'puntaje': 0.4374901583780595,\n",
    "  'shape': (100, 16),\n",
    "  'hijos': None,\n",
    "  'columnas': [12, 13, 14, 15, 58, 59, 60, 72, 73, 74, 78, 80, 81, 84, 85, 88],\n",
    "  'W': None,\n",
    "  'H': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleccionar_caracteristicas(lista_nodos, caracteristicas):\n",
    "  retorno = list()\n",
    "  for elemento in lista_nodos:\n",
    "    actual = dict()\n",
    "    for x in caracteristicas:\n",
    "      actual[x] = elemento[x]\n",
    "    retorno.append(actual)\n",
    "  return retorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seleccionar_caracteristicas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fe92028893a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlista_arbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseleccionar_caracteristicas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_arbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hijos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'destacadas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'seleccionar_caracteristicas' is not defined"
     ]
    }
   ],
   "source": [
    "lista_arbol = seleccionar_caracteristicas(lista_arbol, ['id', 'parent', 'hijos', 'destacadas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lista_arbol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c685673f2b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_arbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lista_arbol' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(lista_arbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
